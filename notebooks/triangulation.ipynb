{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triangulation\n",
    "\n",
    "Given 2D correspondences in images, get the 3D point in the world\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as rot\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure environment\n",
    "\n",
    "<center>\n",
    "    <img src=\"./figures/triangulation-cams.PNG\" alt=\"Cameras in world\" width=300 />\n",
    "</center>\n",
    "\n",
    "- Position two cameras in the scene\n",
    "- Create a demo point which will be captured by the two cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera positions\n",
    "cam1_pos = np.array([1, 1, 2])\n",
    "cam2_pos = np.array([1, 3, 2])\n",
    "demo_pt = np.array([2., 2., 1.])    # Actual point in world (corres)\n",
    "# Camera intrinsic matrices (3x3 shape)\n",
    "f = 3.5e2\n",
    "cx, cy = 300, 300   # 600, 600 image shape\n",
    "cam1_K = np.array([\n",
    "    [f, 0, cx],\n",
    "    [0, f, cy],\n",
    "    [0, 0, 1]], float)\n",
    "cam2_K = cam1_K.copy()\n",
    "# Rotation of cameras (1 & 2) w.r.t. world (extrinsic)\n",
    "r1 = rot.from_euler(\"yz\", [10, 30], degrees=True).as_matrix()\n",
    "r2 = rot.from_euler(\"yz\", [10, -30], degrees=True).as_matrix()\n",
    "# XYZ directions of camera is different\n",
    "rc = np.array([ # Cam XYZ axis w.r.t. world XYZ\n",
    "    [0., 0., 1.],\n",
    "    [-1., 0., 0.],\n",
    "    [0., -1., 0.]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera Projection Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the camera projection model\n",
    "\n",
    "<center>\n",
    "    <img src=\"./figures/triangulation-camproj.PNG\" alt=\"Camera projection model\" width=300 />\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera Extrinsic\n",
    "\n",
    "For handling conversions between different representations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Lambda functions for helping\n",
    "# Position to 4x4 homogeneous transformation\n",
    "pos2ht = lambda pos: np.concatenate((\n",
    "        np.concatenate((np.eye(3), pos.reshape(-1, 1)), axis=1),\n",
    "        np.array([[0, 0, 0, 1]])))\n",
    "# Rotation matrix to 4x4 homogeneous transformation\n",
    "rotm2ht = lambda rotm: np.concatenate((\n",
    "        np.concatenate((rotm, np.zeros((1, 3)))),\n",
    "        np.array([[0, 0, 0, 1]]).T), axis=1)\n",
    "# Position & rotation matrix from 4x4 homogeneous transformation\n",
    "ht2pos = lambda ht: ht[0:3, 3]\n",
    "ht2rotm = lambda ht: ht[0:3, 0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation of camera in world frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {cam} (1 and 2) in {world} -> ^W_C T\n",
    "tf_cam1 = pos2ht(cam1_pos) @ rotm2ht(r1) @ rotm2ht(rc)\n",
    "tf_cam2 = pos2ht(cam2_pos) @ rotm2ht(r2) @ rotm2ht(rc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera Projection function\n",
    "\n",
    "Function to project points using camera model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera projection: KR[I|-X_c]\n",
    "def cam_project(cam_tf, cam_K, X_pts):\n",
    "    \"\"\"\n",
    "        Project a set of points using camera transformation and the\n",
    "        intrinsic properties of the camera.\n",
    "\n",
    "        Parameters:\n",
    "        - cam_tf: The 4x4 camera transform (in world frame)\n",
    "        - cam_K: The 3x3 camera intrinsic matrix\n",
    "        - X_pts: 3D points in the world. Shape: (N, 3) or (3,)\n",
    "\n",
    "        Returns:\n",
    "        - x_pix: Pixel coordinates (x, y) in the image. Shape (N, 2)\n",
    "    \"\"\"\n",
    "    # Extract camera extrinsic properties\n",
    "    X_cam = ht2pos(cam_tf)  # Position of camera in world\n",
    "    R_world_cam = ht2rotm(cam_tf) # {camera} in {world} (Rot. Matrix)\n",
    "    R_cam = R_world_cam.T   # {world} in {camera} (vect. projection)\n",
    "    I_Xc = np.concatenate((np.eye(3), -X_cam.reshape(3, 1)), axis=1)\n",
    "    P = cam_K @ R_cam @ I_Xc\n",
    "    # Convert world points to homogeneous\n",
    "    X_3d = X_pts.reshape(-1, 3).T   # Points as columns: (3, N)\n",
    "    X_hpts = np.concatenate((X_3d, np.ones((1, X_3d.shape[1]))))\n",
    "    # Project world points in the camera\n",
    "    x_hpx = P @ X_hpts\n",
    "    # De-homogenize pixel coordinates\n",
    "    proj_pts = x_hpx[:2]/x_hpx[2]\n",
    "    x_pix = proj_pts.T   # (2, N) -> (N, 2)\n",
    "    return x_pix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## World Point & Correspondence\n",
    "\n",
    "Project a world point (get images) and get the correspondence in the images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captured the point\n",
      "\tImage 1: [[215 472]]\n",
      "\tImage 2: [[384 472]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Project a world point\n",
    "c1_demo = cam_project(tf_cam1, cam1_K, demo_pt)\n",
    "c2_demo = cam_project(tf_cam2, cam2_K, demo_pt)\n",
    "# Pixel coordinates (as observed). Don't expose `demo_pt`\n",
    "p_img1 = c1_demo.astype(int)\n",
    "p_img2 = c2_demo.astype(int)\n",
    "print(f\"Captured the point\"\"\\n\" \\\n",
    "        f\"\\tImage 1: {p_img1}\"\"\\n\" \\\n",
    "        f\"\\tImage 2: {p_img2}\"\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pixel correspondence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to (u, v) pixel (as floats) and then homogeneous\n",
    "p1 = p_img1.astype(float).flatten()\n",
    "p2 = p_img2.astype(float).flatten()\n",
    "p1_h = np.concatenate((p1.reshape(-1, 1), [[1]]))\n",
    "p2_h = np.concatenate((p2.reshape(-1, 1), [[1]]))\n",
    "# Rotation matrix for camera in world\n",
    "R_world_cam1 = ht2rotm(tf_cam1)\n",
    "R_world_cam2 = ht2rotm(tf_cam2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Triangulation\n",
    "\n",
    "Project rays outside the cameras and see where they meet in the real world. These ray may not meet, in which case, we can find the mid-point of the smallest line segment joining these two rays.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using camera model (inverse) to project pixels to rays in 3D world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project to vectors in the world\n",
    "vect1 = R_world_cam1 @ np.linalg.inv(cam1_K) @ p1_h\n",
    "vect2 = R_world_cam2 @ np.linalg.inv(cam2_K) @ p2_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above are vectors, let's normalize them so that they become _directions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to (X, Y, Z) flattened vectors\n",
    "v1 = (vect1 / np.linalg.norm(vect1)).flatten()\n",
    "v2 = (vect2 / np.linalg.norm(vect2)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance minimization\n",
    "\n",
    "Algorithm to minimize the distance between two rays.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theory\n",
    "\n",
    "Refer to the diagram below. Note that the two lines may not intersect (they're rays in 3D).\n",
    "\n",
    "<center>\n",
    "    <img src=\"./figures/triangulation-rays.PNG\" alt=\"Triangulation through intersection of rays\" width=300 />\n",
    "</center>\n",
    "\n",
    "We need to find the $\\alpha, \\beta$ that minimizes $|d|$ (or $d^2$). Basically,\n",
    "\n",
    "$$\n",
    "\\alpha^*, \\beta^* = \\underset{\\alpha, \\beta}{\\textup{argmin}}\\, \\left \\| (\\mathbf{x}_2 + \\beta \\mathbf{v}_2) - (\\mathbf{x}_1 + \\alpha \\mathbf{v}_1) \\right \\|_2^2\n",
    "$$\n",
    "\n",
    "Theoretically, you could solve this directly by $\\bigtriangledown = 0$ (solve two equations and two variables; first derivative of $d^2$ is $0$), but we'll use optimization here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate $d^2 = \\left \\| (\\mathbf{x}_2 + \\beta \\mathbf{v}_2) - (\\mathbf{x}_1 + \\alpha \\mathbf{v}_1) \\right \\|_2^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize distance between the projected points\n",
    "def l2_dist_func(ab_vals, pos_cam1, vect1, pos_cam2, vect2):\n",
    "    \"\"\"\n",
    "        L2 squared distance function to minimize.\n",
    "        - ab_vals: (alpha, beta): Scaling along respective vectors\n",
    "        - pos_cam1: (x, y, z) Position of camera 1\n",
    "        - vect1: 3D direction vector from camera 1\n",
    "        - pos_cam2: (x, y, z) Position of camera 2\n",
    "        - vect2: 3D direction vector from camera 2\n",
    "    \"\"\"\n",
    "    # alpha, beta (for camera 1 and camera 2)\n",
    "    al, be = ab_vals[0], ab_vals[1]\n",
    "    # Get points\n",
    "    pt1 = pos_cam1 + al * vect1\n",
    "    pt2 = pos_cam2 + be * vect2\n",
    "    # Convert to distance (cost)\n",
    "    dvect = (pt2 - pt1)\n",
    "    dist_sq = dvect[0]**2 + dvect[1]**2 + dvect[2]**2\n",
    "    return dist_sq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimization using _Sequential Least Squares Programming_ (SLSQP) in scipy. Using [scipy.optimize.minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize) here, but SLSQP is also available in [pyOpt](http://www.pyopt.org/reference/optimizers.slsqp.html#pySLSQP). We will use the [scipy method](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html#optimize-minimize-slsqp) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get camera poses from the extrinsic properties\n",
    "pos_cam1 = ht2pos(tf_cam1)\n",
    "pos_cam2 = ht2pos(tf_cam2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use \"Sequential Least Squares Programming\" (SLSQP) to minimize\n",
    "res = minimize(l2_dist_func, (0, 0), method=\"SLSQP\",\n",
    "    args=(pos_cam1, v1, pos_cam2, v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the points on the lines\n",
    "alpha, beta = res.x\n",
    "pt1 = pos_cam1 + alpha * v1\n",
    "pt2 = pos_cam2 + beta * v2\n",
    "pt_mid = (pt1 + pt2)/2  # Mid-point is the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point 1: [1.99858352 2.00240966 1.00130448]\n",
      "Point 2: [1.99966674 2.00243211 1.00238856]\n",
      "Achieved end point: [1.99912513 2.00242088 1.00184652]\n",
      "Actual point: [2. 2. 1.]\n",
      "\tError: 0.003167919310173659\n"
     ]
    }
   ],
   "source": [
    "# Error from demo\n",
    "dist_demo = np.linalg.norm(demo_pt - pt_mid)\n",
    "print(f\"Point 1: {pt1}\"\"\\n\" \\\n",
    "    f\"Point 2: {pt2}\"\"\\n\" \\\n",
    "    f\"Achieved end point: {pt_mid}\"\"\\n\" \\\n",
    "    f\"Actual point: {demo_pt}\"\"\\n\" \\\n",
    "    f\"\\tError: {dist_demo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the following out\n",
    "\n",
    "1. Try getting an error map of the 3D space (by sampling many points in a finite volume). What does the visualization say? Is the error more for closer points or far off points?\n",
    "2. Try direct solution (without optimization) and other optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('rrc-work')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be3ce12943ed5c136bd9c9f6c51010b5aaaa4f65a3e3e552d452db549d8b9f33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
